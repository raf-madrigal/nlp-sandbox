{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raf-madrigal/nlp-sandbox/blob/main/data_cleaning_using_textanalyzer_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text EDA\n",
        "- Distribution of text lenghts\n",
        "- Word count\n",
        "- Distribution of top unigrams before removing stopwords (1, 2, 3(\n",
        "- after removing stopwords\n",
        "- Distribution of POS Tags\n",
        "- Review lenght per cateogry\n",
        "- sentiment distribution\n",
        "-"
      ],
      "metadata": {
        "id": "7SJSaD4ouoU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_words(df['Review Text'], 20)\n",
        "for word, freq in common_words:\n",
        "    print(word, freq)\n",
        "df1 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
        "df1.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in review before removing stop words')\n"
      ],
      "metadata": {
        "id": "QmjWuiGkumQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FF9nvCJUu12b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zn9G8eAafN_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "68854309-2126-4426-a2f6-86672abbbbbd"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_en = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "CONTRACTIONS = {\n",
        "    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "class TextAnalyzer:\n",
        "    \"\"\"Class contains methods that does text analysis\"\"\"\n",
        "\n",
        "    def __init__(self, stopwords=[], clean=False, remove_digits=True,\n",
        "                 min_wordlen=2):\n",
        "        \"\"\"Constructor method\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        stopwords: set/list\n",
        "            list of stopwords\n",
        "        clean : bool\n",
        "            Flag whether to run cleaning methods\n",
        "        remove_digits : bool\n",
        "            Whether to remove digits\n",
        "\n",
        "        \"\"\"\n",
        "        self.clean = clean\n",
        "        self.remove_digits = remove_digits\n",
        "        self.min_wordlen = min_wordlen\n",
        "        self.stopwords = set(nltk.corpus.stopwords.words('english') +\n",
        "                             stopwords)\n",
        "\n",
        "    def fit(self, text_file):\n",
        "        self.text = text_file\n",
        "        if self.clean:\n",
        "            self._expand_contractions()\n",
        "            self._remove_ownership()\n",
        "            self._remove_comments_in_parenthesis()\n",
        "            self._remove_accented()\n",
        "            self._remove_punctuation()\n",
        "            self._remove_special(remove_digits=self.remove_digits)\n",
        "            self._lowercase_all()\n",
        "            self._remove_stopwords()\n",
        "            self._remove_short_words(min_wordlen=self.min_wordlen)\n",
        "\n",
        "\n",
        "        else:\n",
        "            self.text = '\\n'.join(list(filter(lambda x: x.strip(), self.text)))\n",
        "\n",
        "        return self\n",
        "    def _remove_short_words(self, min_wordlen):\n",
        "        s = self.text\n",
        "        self.text = (' '.join([word for word in s.split()\n",
        "                                   if len(word)>=min_wordlen]).strip())\n",
        "    def _remove_comments_in_parenthesis(self):\n",
        "        s = self.text\n",
        "        self.text = re.sub(r'\\([^)]*\\)', '', s)\n",
        "\n",
        "    def _remove_ownership(self):\n",
        "        s = self.text\n",
        "        self.text = re.sub(r\"'s\\b\", \"\", s)\n",
        "\n",
        "    def _expand_contractions(self, contractions_dict=CONTRACTIONS):\n",
        "        s = self.text\n",
        "        contractions_re = re.compile(f'{\"|\".join(contractions_dict.keys())}')\n",
        "\n",
        "        def replace(match):\n",
        "            return contractions_dict[match.group(0)]\n",
        "\n",
        "        self.text = (' '.join([contractions_re.sub(replace, x, re.I)\n",
        "                                if x in contractions_dict.keys()\n",
        "                                else x\n",
        "                                for x in s.split(' ')]))\n",
        "\n",
        "    def _remove_accented(self):\n",
        "        text = self.text\n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
        "        text = (unicodedata.normalize('NFKD', text)\n",
        "                           .encode('ascii', 'ignore')\n",
        "                           .decode('utf-8', 'ignore'))\n",
        "        self.text = text\n",
        "\n",
        "    def _remove_special(self, remove_digits=True):\n",
        "        s = self.text\n",
        "        if remove_digits:\n",
        "            self.text = re.sub(r'[^a-zA-Z]', ' ', s)\n",
        "        else:\n",
        "            self.text = re.sub(r'[^a-zA-Z0-9]', ' ', s)\n",
        "\n",
        "\n",
        "    def _join_sentences(self):\n",
        "        \"\"\"Internal method for joining sentences\n",
        "\n",
        "        Checks and groups lines/sentences per paragraph. Paragraphs\n",
        "        are separated by '\\n'.\n",
        "        \"\"\"\n",
        "\n",
        "        par_list = self.text.split('\\n')\n",
        "\n",
        "        sentences = []\n",
        "        paragraph = []\n",
        "        for par in par_list:\n",
        "            sentences = ' '.join(par.split()).strip()\n",
        "            paragraph.append(sentences)\n",
        "\n",
        "        self.text = '\\n'.join(par_list).strip()\n",
        "\n",
        "    def _remove_punctuation(self, punct=string.punctuation):\n",
        "        \"\"\"Internal method for removing punctuations\n",
        "\n",
        "        Punctuations marks are defined by globally using the\n",
        "        list PUNCTUATION_MARKS. This method checks for each\n",
        "        punctuation and replaces them as an empty string\n",
        "        \"\"\"\n",
        "        text = self.text\n",
        "        # punct = [i for i in punct if i is not '.']\n",
        "        self.text = text.translate(str.maketrans(punct,\n",
        "                                     ''*len(punct)))\n",
        "\n",
        "    def _remove_stopwords(self):\n",
        "        \"\"\"Internal method for removing stopwords\"\"\"\n",
        "\n",
        "        par_list = []\n",
        "        for par in self.text.split('\\n'):\n",
        "            sent = ' '.join([word for word in par.split()\n",
        "                                  if word not in self.stopwords])\n",
        "            par_list.append(sent)\n",
        "\n",
        "        self.text = '\\n'.join(par_list).strip()\n",
        "\n",
        "    def _lowercase_all(self):\n",
        "        \"\"\"Internal method for converting text to lower case\"\"\"\n",
        "        self.text = self.text.lower()\n",
        "\n",
        "    def stem(self):\n",
        "        text = self.text\n",
        "        ps = nltk.porter.PorterStemmer()\n",
        "        text = ' '.join([ps.stem(w) for w in nltk.word_tokenize(text)])\n",
        "        self.text = text\n",
        "        return self\n",
        "\n",
        "    def lemmatize(self):\n",
        "        pars = self.text\n",
        "\n",
        "        def get_wordnet_pos(word):\n",
        "            \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "            tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "            tag_dict = {\"J\": wordnet.ADJ,\n",
        "                        \"N\": wordnet.NOUN,\n",
        "                        \"V\": wordnet.VERB,\n",
        "                        \"R\": wordnet.ADV}\n",
        "\n",
        "            return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        par_compile = []\n",
        "        for par in pars.split('\\n'):\n",
        "            par_compile.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(par)]))\n",
        "        self.text = '\\n'.join(par_compile)\n",
        "        return self\n",
        "\n",
        "    def ngrams(self, n=1):\n",
        "        \"\"\"Returns the n-gram of the text.\n",
        "\n",
        "        An ð‘›-gram is a sequence of ð‘› successive words in a paragraph of text.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n : int\n",
        "            Number of successive words, default value is 1\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Dict-like object with the ð‘›-gram string as keys and\n",
        "            their counts as values\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> analyzer = analyzer = PGalyzer(file_path, True)\n",
        "        >>> analyzer.ngram(2)\n",
        "        {'produced by': 1,\n",
        "         'by greg': 1,\n",
        "         'greg weeks': 1,\n",
        "         'weeks dave': 1,\n",
        "         ...}\n",
        "        \"\"\"\n",
        "        par_list = self.text.split('\\n')\n",
        "\n",
        "        n_words = []\n",
        "\n",
        "        for par in par_list:\n",
        "            word_list = par.split()\n",
        "            if len(word_list) >= n:\n",
        "                for i in range(len(word_list)-n+1):\n",
        "                    n_words.append(' '.join(word_list[i:i+n]))\n",
        "\n",
        "\n",
        "        return dict(Counter(n_words).most_common())\n",
        "\n",
        "    def word_count(self):\n",
        "        \"\"\"Returns the word count of the text\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Dict-like object with the word as keys and the frequency\n",
        "            as values.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> analyzer = analyzer = PGalyzer(file_path, True)\n",
        "        >>> analyzer.wordcount\n",
        "        {'produced': 4,\n",
        "         'by': 14,\n",
        "         'greg': 1,\n",
        "         'weeks': 2,\n",
        "         'dave': 1,\n",
        "         ...}\n",
        "        \"\"\"\n",
        "        return dict(Counter(self.text.split()).most_common())\n",
        "\n",
        "    def concordance(self, word, neighborhood_size=10):\n",
        "        \"\"\"Gets the left and right neighbors of a word\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        word : str\n",
        "            The string to be processed in getting the concordance\n",
        "        neighborhood_size : int, optional\n",
        "            Number of words to check from the word's left\n",
        "            and right neighbors (Defaults to 10)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        con_list : list\n",
        "            List of tuples containing the left and\n",
        "            right neighbors of the argument word\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> analyzer = analyzer = PGalyzer(file_path, True)\n",
        "        >>> analyzer.concordance('the', 2)\n",
        "        [('blundell and', 'online distributed'),\n",
        "         ('he was', 'first to'),\n",
        "         ('of it', 'doctor fumbled')\n",
        "         ...]\n",
        "        \"\"\"\n",
        "        word_list = self.text.split('\\n')\n",
        "        con_list = []\n",
        "        for string in word_list:\n",
        "            str_list = string.split()\n",
        "            indexes = [i for i, e in enumerate(str_list) if e == word]\n",
        "            for i in indexes:\n",
        "                if i-neighborhood_size >= 0:\n",
        "                    string_before = ' '.join(str_list[i-neighborhood_size:i])\n",
        "                else:\n",
        "                    string_before = ' '.join(str_list[:i])\n",
        "\n",
        "                if len(word_list) - (neighborhood_size+1) >= 0:\n",
        "                    string_after = ' '.join(str_list[i+1:\n",
        "                                                     i+neighborhood_size+1])\n",
        "                else:\n",
        "                    string_after = ' '.join(str_list[i+1:])\n",
        "\n",
        "                con_list.append((string_before, string_after))\n",
        "\n",
        "        return con_list\n",
        "\n",
        "    def display_concordance(self, word, neighborhood_size=10):\n",
        "        \"\"\"Displays the concordance with the word in bold format\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        word : str\n",
        "            The string to be processed in getting the concordance\n",
        "        neighborhood_size : int, optional\n",
        "            Number of words to check from the word's left\n",
        "            and right neighbors (Defaults to 10)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str\n",
        "            String of concordance vertically aligned on the argument\n",
        "            word with the argument word in a bold format\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> analyzer = analyzer = PGalyzer(file_path, True)\n",
        "        >>> from IPython.core.display import HTML\n",
        "        >>> HTML(analyzer.display_concordance('boys', 2))\n",
        "        <pre>anything the <b>boys</b> upstairs try\n",
        "        lung cancer <b>boys</b> used to\n",
        "        wordand the <b>boys</b> in the</pre>\n",
        "        \"\"\"\n",
        "        ccd = self.concordance(word, neighborhood_size)\n",
        "        left_padding = max(map(lambda x: len(x[0]), ccd))\n",
        "        text_list = []\n",
        "\n",
        "        bold_word = '<b>'+word+'</b>'\n",
        "        for i in ccd:\n",
        "            text_list.append(f'{\" \" * ( left_padding - len(i[0]) )}{i[0]} '\n",
        "                             f'{bold_word} {i[1]}')\n",
        "        return '<pre>'+'\\n'.join(text_list)+'</pre>'\n",
        "\n",
        "    def likely_next(self, word, n=5):\n",
        "        \"\"\"Counter of words that are next to another word\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        word : str\n",
        "            String to check for the words that are likely next to it.\n",
        "        n : int\n",
        "            Top n likely next words to retrieve\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Counter\n",
        "            Counter of top n strings that are likely next to the argument word.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> analyzer = analyzer = PGalyzer(file_path, True)\n",
        "        >>> analyzer.likely_next('of')\n",
        "        [('the', 30), ('his', 5), ('it', 5), ('that', 4),\n",
        "        (\"thurston's\", 4)]\n",
        "        \"\"\"\n",
        "\n",
        "        word_list = self.text.split('\\n')\n",
        "        d_likely_next = defaultdict(Counter)\n",
        "\n",
        "        for wl in word_list:\n",
        "            tokenized = wl.split()\n",
        "\n",
        "            for idx, v in enumerate(tokenized[:-1]):\n",
        "                d_likely_next[v].update([tokenized[idx+1]])\n",
        "\n",
        "        return sorted(d_likely_next[word].items(),\n",
        "                      key=lambda x: (-x[1], x[0]))[:n]\n",
        "\n",
        "    def likely_previous(self, word, n=5):\n",
        "        \"\"\"Counter of words that are previous to another word\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        word : str\n",
        "            String to check for the words that are likely previous to it.\n",
        "        n : int\n",
        "            Top n likely previous words to retrieve\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Counter\n",
        "            Counter of top n strings that are likely previous to the\n",
        "            argument word.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> analyzer = analyzer = PGalyzer(file_path, True)\n",
        "        >>> analyzer.likely_previous('of')\n",
        "        [('out', 7), ('one', 6), ('cent', 4), ('end', 4), ('cloud', 3)]\n",
        "        \"\"\"\n",
        "        word_list = self.text.split('\\n')\n",
        "        d_likely_previous = defaultdict(Counter)\n",
        "\n",
        "        for wl in word_list:\n",
        "            tokenized = wl.split()\n",
        "\n",
        "            for idx, v in enumerate(tokenized[1:]):\n",
        "                d_likely_previous[v].update([tokenized[idx]])\n",
        "\n",
        "        return sorted(d_likely_previous[word].items(),\n",
        "                      key=lambda x: (-x[1], x[0]))[:n]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b5ab3e9e1f19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTextAnalyzer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;34m\"\"\"Class contains methods that does text analysis\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-b5ab3e9e1f19>\u001b[0m in \u001b[0;36mTextAnalyzer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_remove_short_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_wordlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_wordlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         self.text = (' '.join([word for word in s.split() \n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VtM61KadJWg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "root = '/content/drive/My Drive/Colab Notebooks/ml1/final_project'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lh5glSxbPmB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "802a90bb-c66b-40f5-f656-3a8edcf49a17"
      },
      "source": [
        "\n",
        "\n",
        "articles = pd.read_csv(f'{root}/data/articles_v11.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e8d8398eb920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/Colab Notebooks/ml1/final_project'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{root}/data/articles_v11.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /content/drive/My Drive/Colab Notebooks/ml1/final_project/data/articles_v11.csv does not exist: '/content/drive/My Drive/Colab Notebooks/ml1/final_project/data/articles_v11.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iciFBFQbp__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "ef401822-5c52-4d40-8712-74c4e355c0a0"
      },
      "source": [
        "articles.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>category</th>\n",
              "      <th>author</th>\n",
              "      <th>published_date</th>\n",
              "      <th>updated_date</th>\n",
              "      <th>title</th>\n",
              "      <th>metadesc</th>\n",
              "      <th>happy</th>\n",
              "      <th>sad</th>\n",
              "      <th>angry</th>\n",
              "      <th>dontcare</th>\n",
              "      <th>inspired</th>\n",
              "      <th>afraid</th>\n",
              "      <th>amused</th>\n",
              "      <th>annoyed</th>\n",
              "      <th>raw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://r3.rappler.com/nation/238493-preemptiv...</td>\n",
              "      <td>nation</td>\n",
              "      <td>Janella Paris</td>\n",
              "      <td>2019-08-24 23:46:00</td>\n",
              "      <td>2019-10-03 21:58:00</td>\n",
              "      <td>Preemptive measures 'spared QC from Ineng floo...</td>\n",
              "      <td>Dredging and declogging activities ordered by ...</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>MANILA, Philippines â€“ The Quezon City governme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://r3.rappler.com/life-and-style/specials...</td>\n",
              "      <td>life-and-style</td>\n",
              "      <td>Rappler.com</td>\n",
              "      <td>2019-11-15 13:23:00</td>\n",
              "      <td>2019-11-15 13:23:00</td>\n",
              "      <td>WATCH: Emma Tiglao talks about pageant journey...</td>\n",
              "      <td>Emma is aiming for a back-to-back win in the M...</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>MANILA, Philippines â€“ The introduction video o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://r3.rappler.com/sports/university/uaap/...</td>\n",
              "      <td>sports</td>\n",
              "      <td>JR Isaga</td>\n",
              "      <td>2019-10-07 18:44:00</td>\n",
              "      <td>2019-10-07 18:44:00</td>\n",
              "      <td>'Marked men' Ahanmisi, Chauca not panicking am...</td>\n",
              "      <td>The Adamson Soaring Falcons have lost 3 straig...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>MANILA, Philippines â€“ It seems not too long ag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://r3.rappler.com/business/246266-naia-te...</td>\n",
              "      <td>business</td>\n",
              "      <td>Loreben Tuquero</td>\n",
              "      <td>2019-12-02 20:30:00</td>\n",
              "      <td>2019-12-02 20:30:00</td>\n",
              "      <td>NAIA terminals to close for 12 hours on Decemb...</td>\n",
              "      <td>Stranded passengers are advised to stay home a...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>MANILA, Philippines â€“ The terminals at the Nin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://r3.rappler.com/world/regions/asia-paci...</td>\n",
              "      <td>world</td>\n",
              "      <td>Agence France-Presse</td>\n",
              "      <td>2019-08-20 10:46:00</td>\n",
              "      <td>2019-08-20 10:46:00</td>\n",
              "      <td>Father of victim hopes for justice as Cardinal...</td>\n",
              "      <td>Australian Cardinal George Pell, the former Va...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>SYDNEY, Australia â€“ The father of one of the v...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                link  ...                                                raw\n",
              "0  https://r3.rappler.com/nation/238493-preemptiv...  ...  MANILA, Philippines â€“ The Quezon City governme...\n",
              "1  https://r3.rappler.com/life-and-style/specials...  ...  MANILA, Philippines â€“ The introduction video o...\n",
              "2  https://r3.rappler.com/sports/university/uaap/...  ...  MANILA, Philippines â€“ It seems not too long ag...\n",
              "3  https://r3.rappler.com/business/246266-naia-te...  ...  MANILA, Philippines â€“ The terminals at the Nin...\n",
              "4  https://r3.rappler.com/world/regions/asia-paci...  ...  SYDNEY, Australia â€“ The father of one of the v...\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTlTjTs87fvM"
      },
      "source": [
        "tx = TextAnalyzer(stop_words=stop_en, clean=True, remove_digits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_wzfpUTb2AK"
      },
      "source": [
        "\n",
        "articles['title_clean'] = articles.title.apply(lambda x : tx.fit(x).lemmatize().text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xng6wUatlnak"
      },
      "source": [
        "articles['metadesc_clean'] = articles.metadesc.apply(lambda x : tx.fit(x).lemmatize().text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txSvFP_TlpUn"
      },
      "source": [
        "def helper(text):\n",
        "    return re.sub(r'\\b([A-Z]+,[\\w\\s]+â€“)\\s?|â€“ Rappler.com', '', text)\n",
        "\n",
        "articles['raw_clean'] = articles.raw.apply(lambda x : tx.fit(helper(x)).lemmatize().text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAFeJldPd1Wx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "8be3d5bd-532c-46b7-c360-3d52abc7606d"
      },
      "source": [
        "articles.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>category</th>\n",
              "      <th>author</th>\n",
              "      <th>published_date</th>\n",
              "      <th>updated_date</th>\n",
              "      <th>title</th>\n",
              "      <th>metadesc</th>\n",
              "      <th>happy</th>\n",
              "      <th>sad</th>\n",
              "      <th>angry</th>\n",
              "      <th>dontcare</th>\n",
              "      <th>inspired</th>\n",
              "      <th>afraid</th>\n",
              "      <th>amused</th>\n",
              "      <th>annoyed</th>\n",
              "      <th>raw</th>\n",
              "      <th>title_clean</th>\n",
              "      <th>metadesc_clean</th>\n",
              "      <th>raw_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://r3.rappler.com/nation/238493-preemptiv...</td>\n",
              "      <td>nation</td>\n",
              "      <td>Janella Paris</td>\n",
              "      <td>2019-08-24 23:46:00</td>\n",
              "      <td>2019-10-03 21:58:00</td>\n",
              "      <td>Preemptive measures 'spared QC from Ineng floo...</td>\n",
              "      <td>Dredging and declogging activities ordered by ...</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>MANILA, Philippines â€“ The Quezon City governme...</td>\n",
              "      <td>preemptive measure spar qc ineng flood city gov</td>\n",
              "      <td>dredge declogging activity order quezon city m...</td>\n",
              "      <td>quezon city government report late saturday ni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://r3.rappler.com/life-and-style/specials...</td>\n",
              "      <td>life-and-style</td>\n",
              "      <td>Rappler.com</td>\n",
              "      <td>2019-11-15 13:23:00</td>\n",
              "      <td>2019-11-15 13:23:00</td>\n",
              "      <td>WATCH: Emma Tiglao talks about pageant journey...</td>\n",
              "      <td>Emma is aiming for a back-to-back win in the M...</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>MANILA, Philippines â€“ The introduction video o...</td>\n",
              "      <td>watch emma tiglao talk pageant journey charity...</td>\n",
              "      <td>emma aim back back win miss intercontinental p...</td>\n",
              "      <td>introduction video emma tiglao philippine repr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://r3.rappler.com/sports/university/uaap/...</td>\n",
              "      <td>sports</td>\n",
              "      <td>JR Isaga</td>\n",
              "      <td>2019-10-07 18:44:00</td>\n",
              "      <td>2019-10-07 18:44:00</td>\n",
              "      <td>'Marked men' Ahanmisi, Chauca not panicking am...</td>\n",
              "      <td>The Adamson Soaring Falcons have lost 3 straig...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>MANILA, Philippines â€“ It seems not too long ag...</td>\n",
              "      <td>marked men ahanmisi chauca panic amid adamson ...</td>\n",
              "      <td>adamson soar falcon lose straight game win fir...</td>\n",
              "      <td>seem long ago adamson soar falcon cusp reach u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://r3.rappler.com/business/246266-naia-te...</td>\n",
              "      <td>business</td>\n",
              "      <td>Loreben Tuquero</td>\n",
              "      <td>2019-12-02 20:30:00</td>\n",
              "      <td>2019-12-02 20:30:00</td>\n",
              "      <td>NAIA terminals to close for 12 hours on Decemb...</td>\n",
              "      <td>Stranded passengers are advised to stay home a...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>MANILA, Philippines â€“ The terminals at the Nin...</td>\n",
              "      <td>naia terminal close hour december due typhoon ...</td>\n",
              "      <td>strand passenger advise stay home avoid go air...</td>\n",
              "      <td>terminal ninoy aquino international airport na...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://r3.rappler.com/world/regions/asia-paci...</td>\n",
              "      <td>world</td>\n",
              "      <td>Agence France-Presse</td>\n",
              "      <td>2019-08-20 10:46:00</td>\n",
              "      <td>2019-08-20 10:46:00</td>\n",
              "      <td>Father of victim hopes for justice as Cardinal...</td>\n",
              "      <td>Australian Cardinal George Pell, the former Va...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>SYDNEY, Australia â€“ The father of one of the v...</td>\n",
              "      <td>father victim hope justice cardinal pell wait ...</td>\n",
              "      <td>australian cardinal george pell former vatican...</td>\n",
              "      <td>father one victim jail australian cardinal geo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                link  ...                                          raw_clean\n",
              "0  https://r3.rappler.com/nation/238493-preemptiv...  ...  quezon city government report late saturday ni...\n",
              "1  https://r3.rappler.com/life-and-style/specials...  ...  introduction video emma tiglao philippine repr...\n",
              "2  https://r3.rappler.com/sports/university/uaap/...  ...  seem long ago adamson soar falcon cusp reach u...\n",
              "3  https://r3.rappler.com/business/246266-naia-te...  ...  terminal ninoy aquino international airport na...\n",
              "4  https://r3.rappler.com/world/regions/asia-paci...  ...  father one victim jail australian cardinal geo...\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba-uWFQGzrsM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "70e7bc15-442a-492a-df9e-8d23e2174c4a"
      },
      "source": [
        "articles.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "link              0\n",
              "category          0\n",
              "author            0\n",
              "published_date    0\n",
              "updated_date      0\n",
              "title             0\n",
              "metadesc          0\n",
              "happy             0\n",
              "sad               0\n",
              "angry             0\n",
              "dontcare          0\n",
              "inspired          0\n",
              "afraid            0\n",
              "amused            0\n",
              "annoyed           0\n",
              "raw               0\n",
              "title_clean       0\n",
              "metadesc_clean    4\n",
              "raw_clean         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd1pas80jBTZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "347aed8e-fa53-4047-84c4-fa4c8e415529"
      },
      "source": [
        "text = articles.raw_clean[0]\n",
        "text.split('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['quezon city government report late saturday night august preemptive measure help spare city massive flood experienced part metro manila follow heavy rain cause southwest monsoon',\n",
              " 'qc government say press statement base assessment fact report massive flood throughout saturday relayed quezon city emergency operation center',\n",
              " 'add qc urban search rescue team place standby receive distress call relation flood',\n",
              " 'accord quezon city disaster risk reduction management office drrmo head karl michael marasigan preemptive measure order qc mayor joy belmonte declogging dredge activity city early july clear city drainage system help prevent flood',\n",
              " 'round clock declogging canal order mayor joy belmonte take advantage two week monsoon break dredge key waterway traverse city since first week belmonte administration facilitate speedy flow rainwater drainage system marasigan say',\n",
              " '',\n",
              " 'qc government note pagasa science garden synoptic station record millimeter rain saturday classify heavy hourly rate mm',\n",
              " 'part metro manila experienced massive flood saturday due part northern philippine ilocos norte affected severe tropical storm ineng bailu']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ8a1TZnjlDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a62798c4-925b-48a5-a449-43c95d5750fd"
      },
      "source": [
        "len(re.findall(r'\\d+', ' '.join(articles.raw_clean.values)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtirsA0MeYJ0"
      },
      "source": [
        "articles.to_csv(f'{root}/data/articles_v7.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAn9iJ5NfODr"
      },
      "source": [
        "articles1 = pd.read_csv(f'{root}/data/articles_v12.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCtP5e2UQ-rw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b103d6fb-2748-40f0-87ac-41eaab333434"
      },
      "source": [
        "articles1[(articles1.happy >= 20) & (articles1.sad >= 15) & (articles1.afraid >= 10) & (articles1.angry >= 10) & (articles1.inspired >= 10)].iloc[0,:].title"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tisoy weakens into tropical depression as it leaves PAR'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6fzDMv97qWd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5952ff13-6b14-42f5-9ec6-55241de7da3f"
      },
      "source": [
        "articles1[(articles1.happy >= 20) & (articles1.sad >= 15) & (articles1.afraid >= 10) & (articles1.angry >= 10) & (articles1.inspired >= 10)].iloc[0,:].metadesc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Though Tisoy (Kammuri) is no longer inside the Philippine Area of Responsibility, there may still be heavy rain due to the surge of the northeast monsoon and the tail-end of a cold front'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r54JclkD1v1T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8dfdc80c-9e32-4243-ebcf-c94902bc8a7d"
      },
      "source": [
        "articles1[(articles1.happy >= 20) & (articles1.sad >= 15) & (articles1.afraid >= 10) & (articles1.angry >= 10) & (articles1.inspired >= 10)].iloc[0,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "link                https://r3.rappler.com/nation/special-coverage...\n",
              "category                                                       nation\n",
              "author                                                     Acor Arceo\n",
              "published_date                                    2019-12-05 12:30:00\n",
              "updated_date                                      2019-12-05 12:30:00\n",
              "title               Tisoy weakens into tropical depression as it l...\n",
              "metadesc            Though Tisoy (Kammuri) is no longer inside the...\n",
              "happy                                                              22\n",
              "sad                                                                33\n",
              "angry                                                              11\n",
              "dontcare                                                            0\n",
              "inspired                                                           11\n",
              "afraid                                                             11\n",
              "amused                                                              1\n",
              "annoyed                                                            11\n",
              "raw                 What's the weather like in your area? Tweet us...\n",
              "title_clean                tisoy weakens tropical depression leaf par\n",
              "metadesc_clean      tisoy kammuri longer inside philippine area re...\n",
              "raw_clean           weather like area tweet rapplerdotcom\\ntisoy k...\n",
              "dominant_emotion                                                  sad\n",
              "Name: 4854, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0bYC42c6skx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4b53456-0bae-4ad9-9b13-be3421673f37"
      },
      "source": [
        "articles1[articles1.raw_clean.isnull()].raw.iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0750. ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfWVRWyw51yo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "febe0876-84ce-4c01-d58b-add4ad87f7a6"
      },
      "source": [
        "replace = {'â€“ Rappler.com', ''}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Autism awareness and mental health are some of the causes closest to this beauty queen's heart\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UApg4ulk6v8P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "a20ea52b-4c9b-4ac8-bfd1-cf06061c8e14"
      },
      "source": [
        "text = articles1.raw.iloc[0]\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MANILA, Philippines â€“ The Quezon City government reported late Saturday night, August 24, that the its preemptive measures helped spare the city from the massive flooding experienced in other parts of Metro Manila, following heavy rains caused by the southwest monsoon.\\nThe QC government said in a press statement that it based this assessment on the fact that \"there were no reports of massive flooding throughout Saturday,\" as relayed by the  Quezon City Emergency Operations Center.\\nIt added that the QC Urban Search and Rescue Team which had been placed on standby did not receive any distress call in relation to flooding.\\nAccording to Quezon City Disaster Risk Reduction and Management Office (DRRMO) head Karl Michael Marasigan, preemptive measures ordered by QC Mayor Joy Belmonte â€“ declogging and dredging activities in the city as early as July â€“ had cleared the city\\'s drainage system and helped prevent flooding.\\nâ€œThe round-the-clock declogging of canals ordered by Mayor Joy Belmonte to take advantage of the two-week monsoon break, and dredging of key waterways traversing the city since the first week of the Belmonte administration facilitated the speedy flow of rainwater in our drainage system,â€ Marasigan said.\\n\\nThe QC government noted that the PAGASA Science Garden Synoptic Station recorded 25.6 millimeters of rain from 8 am to 11 am on Saturday â€“ classified as heavy with an hourly rate of 8.5 mm.\\nOther parts of Metro Manila experienced massive flooding on Saturday due while other parts of the northern Philippines, such as Ilocos Norte, were affected by  Severe Tropical Storm Ineng (Bailu). â€“ Rappler.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUSwOq4BEQpb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "15952ddc-92f7-490f-f473-13f947044be9"
      },
      "source": [
        "re.sub(r'â€“ Rappler.com', '', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'MANILA, Philippines â€“ The Quezon City government reported late Saturday night, August 24, that the its preemptive measures helped spare the city from the massive flooding experienced in other parts of Metro Manila, following heavy rains caused by the southwest monsoon.\\nThe QC government said in a press statement that it based this assessment on the fact that \"there were no reports of massive flooding throughout Saturday,\" as relayed by the  Quezon City Emergency Operations Center.\\nIt added that the QC Urban Search and Rescue Team which had been placed on standby did not receive any distress call in relation to flooding.\\nAccording to Quezon City Disaster Risk Reduction and Management Office (DRRMO) head Karl Michael Marasigan, preemptive measures ordered by QC Mayor Joy Belmonte â€“ declogging and dredging activities in the city as early as July â€“ had cleared the city\\'s drainage system and helped prevent flooding.\\nâ€œThe round-the-clock declogging of canals ordered by Mayor Joy Belmonte to take advantage of the two-week monsoon break, and dredging of key waterways traversing the city since the first week of the Belmonte administration facilitated the speedy flow of rainwater in our drainage system,â€ Marasigan said.\\n\\nThe QC government noted that the PAGASA Science Garden Synoptic Station recorded 25.6 millimeters of rain from 8 am to 11 am on Saturday â€“ classified as heavy with an hourly rate of 8.5 mm.\\nOther parts of Metro Manila experienced massive flooding on Saturday due while other parts of the northern Philippines, such as Ilocos Norte, were affected by  Severe Tropical Storm Ineng (Bailu). '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMqsZq7nEa0R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "341142e5-d3bf-428b-d415-6be5bfa7433a"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Quezon City government reported late Saturday night, August 24, that the its preemptive measures helped spare the city from the massive flooding experienced in other parts of Metro Manila, following heavy rains caused by the southwest monsoon.\\nThe QC government said in a press statement that it based this assessment on the fact that \"there were no reports of massive flooding throughout Saturday,\" as relayed by the  Quezon City Emergency Operations Center.\\nIt added that the QC Urban Search and Rescue Team which had been placed on standby did not receive any distress call in relation to flooding.\\nAccording to Quezon City Disaster Risk Reduction and Management Office (DRRMO) head Karl Michael Marasigan, preemptive measures ordered by QC Mayor Joy Belmonte â€“ declogging and dredging activities in the city as early as July â€“ had cleared the city\\'s drainage system and helped prevent flooding.\\nâ€œThe round-the-clock declogging of canals ordered by Mayor Joy Belmonte to take advantage of the two-week monsoon break, and dredging of key waterways traversing the city since the first week of the Belmonte administration facilitated the speedy flow of rainwater in our drainage system,â€ Marasigan said.\\n\\nThe QC government noted that the PAGASA Science Garden Synoptic Station recorded 25.6 millimeters of rain from 8 am to 11 am on Saturday â€“ classified as heavy with an hourly rate of 8.5 mm.\\nOther parts of Metro Manila experienced massive flooding on Saturday due while other parts of the northern Philippines, such as Ilocos Norte, were affected by  Severe Tropical Storm Ineng (Bailu). '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40XqrjVyErwD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}